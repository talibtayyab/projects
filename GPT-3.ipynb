{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d419804a-e40b-43c6-8d9b-dd91acb90c9e",
   "metadata": {},
   "source": [
    "All imports in one place for better redability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "094dd99f-a518-49bf-85c8-5158098d84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6fe99-09bb-42be-a137-90195cf64935",
   "metadata": {},
   "source": [
    "Custom PyTorch Dataset for GPT-style language modeling.\n",
    "This class takes a large text, tokenizes it, and splits it into overlapping input-target sequence pairs using a sliding window. Each input sequence is shifted by one token to create the target sequence for next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd0006a4-32bc-41f6-aa0c-51243fa0741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT_Dataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2776ef-88cb-4c97-9714-e78434f471e4",
   "metadata": {},
   "source": [
    "Function to create a PyTorch DataLoader for GPT-style training.\n",
    "It tokenizes the input text using the GPT-2 tokenizer, generates overlapping input-target sequences via the GPT_Dataset class, and packages them into batches for efficient model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60d8818-c7f1-4c4b-8e6c-d377493623fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def create_dataloader(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPT_Dataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d9ab8-d89e-4ae4-a8c9-f15255bdc278",
   "metadata": {},
   "source": [
    "Configuration dictionary for a 124M-parameter GPT model.\n",
    "Defines core hyperparameters including vocabulary size, context length, embedding dimensions, attention heads, transformer layers, dropout rate, and query-key-value bias setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd836cd-a000-453e-af41-ba8ee4033f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 256,   # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4476a94-63c5-49af-831c-366654c51147",
   "metadata": {},
   "source": [
    "Utility functions for text–token conversions in GPT models.\n",
    "text_to_token_ids encodes text into token IDs and returns them as a batch tensor.\n",
    "token_ids_to_text decodes token ID tensors back into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4fad3fe-69ed-43b7-a38a-6cf182bdb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672198d0-62cc-4c3f-9be9-82ea90bb227a",
   "metadata": {},
   "source": [
    "Code to load and preprocess the “Moby Dick” text dataset for GPT-2 style training.\n",
    "It reads the text file, tokenizes it using GPT-2 encoding, and extracts key model configuration parameters such as vocabulary size, embedding dimension, context length, and stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3eaa95e-40e2-4fcf-a055-37f0a66dcf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file \"Moby Dick-By Herman Melville.txt\" saved as pg2701.txt in read mode with UTF-8 encoding\n",
    "with open(\"pg2701.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "# Initialize a tokenizer using the GPT-2 encoding scheme\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Encode the text data into token IDs using the GPT-2 tokenizer\n",
    "encoded_text = tokenizer.encode(text_data)\n",
    "\n",
    "# Retrieve configuration parameters for the GPT model (124M variant)\n",
    "vocab_size = GPT_CONFIG_124M[\"vocab_size\"] # Size of the vocabulary (# of unique tokens)\n",
    "output_dim = GPT_CONFIG_124M[\"emb_dim\"] # Dimensionality of the token embeddings\n",
    "context_length = GPT_CONFIG_124M[\"context_length\"] # Maximum sequence length\n",
    "stride=GPT_CONFIG_124M[\"context_length\"] # Use the same context length as the stride\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c304d-194c-4ed8-9902-f7b5faaceedd",
   "metadata": {},
   "source": [
    "Code to set up embeddings and prepare data for GPT-style model training.\n",
    "Defines token and positional embedding layers, generates a DataLoader with overlapping token sequences, and splits the dataset into training and validation subsets based on a specified ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9133f6bb-dbab-4439-94e5-726d5e7386af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding layer to learn vector representations for each token in the vocabulary\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# Create a positional embedding layer to encode the position of tokens in the sequence\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# Generate a dataloader for training by slicing the text into sequences of `max_length` tokens\n",
    "# with a stride (sliding window) to allow overlap, supporting batch training\n",
    "dataloader = create_dataloader(text_data, batch_size=8, max_length=context_length, stride=stride)\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c72c47-dcdc-4aa2-a72d-e8389fa8264c",
   "metadata": {},
   "source": [
    "Code to initialize reproducible training and create the training DataLoader.\n",
    "It fixes the random seed for consistency, then prepares batches of token sequences from the training text using the configured context length and stride, with shuffling enabled and incomplete batches dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92ae557b-9bcb-436c-93a6-69012b034898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility (ensures consistent random behavior across runs)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Create a DataLoader for the training data with specified parameters\n",
    "train_loader = create_dataloader(\n",
    "    train_data,\t\t# The training text data\n",
    "    batch_size=2,\t# Number of sequences per batch\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"], # Length of each token sequence\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"], # Step size for the sliding window -no overlap \n",
    "    drop_last=True,\t# Drop the last batch if it's smaller than batch_size\n",
    "    shuffle=True,\t\t# Shuffle the data at the start of each epoch\n",
    "    num_workers=0\t# Number of subprocesses for data loading (0 = use main process)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2786855-d1b0-4322-aa80-3e0681acf114",
   "metadata": {},
   "source": [
    "Code to create the validation DataLoader.\n",
    "It batches token sequences from the validation text without shuffling, preserves incomplete batches, and uses the model’s configured context length and stride for sequence generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b471aadc-3cf2-46dd-8795-10ed40cfb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = create_dataloader(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574f8b3-65e6-43fc-a83d-be9c5f642607",
   "metadata": {},
   "source": [
    "Function to compute the average cross-entropy loss for a batch during GPT model training.\n",
    "It moves input and target tensors to the specified device, runs a forward pass to get predicted logits, then calculates token-level loss by flattening predictions and targets to align shapes for the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34b3d600-937e-45e3-a00b-b1d3d9b9b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    # Move input and target tensors to the specified device (CPU or GPU)\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "    # Forward pass: get the model's predictions (logits) for the input batch\n",
    "    logits = model(input_batch)\n",
    "\n",
    "    # Compute the cross-entropy loss between the predicted logits and the target tokens\n",
    "    # Reshape logits and targets to 2D tensors:\n",
    "    #   - logits: (batch_size * seq_len, vocab_size)\n",
    "    #   - target_batch: (batch_size * seq_len)\n",
    "    # This is required for token-level classification (e.g., language modeling)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)),   \t# Flatten the logits\n",
    "        target_batch.view(-1),              \t# Flatten the targets\n",
    "        reduction='mean'                    \t# Compute the mean loss over all tokens\n",
    "    )\n",
    "\n",
    "    # Return the scalar loss value\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5379563-a37f-4f64-8fe8-d34084e5fe9c",
   "metadata": {},
   "source": [
    "Function to calculate the average cross-entropy loss over multiple batches from a DataLoader.\n",
    "It processes a specified number of batches (or all if unspecified), sums token-level losses for accuracy, and returns the normalized loss per token, supporting evaluation on training or validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2083d3e7-0bca-4f7b-9941-b8a8c9bab89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.          \t# Accumulates total loss over all batches\n",
    "    total_tokens = 0         \t# Accumulates total number of tokens (used for normalization)\n",
    "\n",
    "    # Edge case: if the DataLoader is empty, return NaN to indicate no loss can be calculated\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    # Determine how many batches to evaluate:\n",
    "    # - If num_batches is not specified, evaluate the entire DataLoader\n",
    "    # - Otherwise, limit evaluation to the smaller of num_batches or total available batches\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            # Move input and target tensors to the specified device (e.g., GPU or CPU)\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            # Forward pass: compute model predictions (logits) for the input batch\n",
    "            logits = model(input_batch)\n",
    "\n",
    "            # Compute cross-entropy loss across all tokens in the batch\n",
    "            # Flattening:\n",
    "            # - logits: shape (batch_size, seq_len, vocab_size) → (batch_size * seq_len, vocab_size)\n",
    "            # - target_batch: shape (batch_size, seq_len) → (batch_size * seq_len)\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits.flatten(0, 1),              # Combine batch and sequence dimensions\n",
    "                target_batch.flatten(),            # Flatten targets to match logits\n",
    "                reduction='sum'                    # Sum token-level losses for accurate total\n",
    "            )\n",
    "\n",
    "            # Accumulate total loss and total number of tokens\n",
    "            total_loss += loss.item()              # Convert loss tensor to scalar and accumulate\n",
    "            total_tokens += target_batch.numel()   # Count total tokens (for averaging later)\n",
    "        else:\n",
    "            break  # Stop iterating if we've processed the desired number of batches\n",
    "\n",
    "    # Return average loss per token (like in training), making it comparable across datasets\n",
    "    return total_loss / total_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c4e68-e552-44ed-8e30-7df176118bb1",
   "metadata": {},
   "source": [
    "Function to train a GPT model over multiple epochs with periodic evaluation.\n",
    "It runs training steps with gradient updates, tracks token counts and losses, evaluates the model on training and validation sets at regular intervals, prints progress updates, and returns loss histories for analysis or plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db93aa9c-38d2-424d-9c89-4599b7fedbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "                       num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track training/validation losses and total tokens processed\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1  # Track total tokens and global training steps\n",
    "\n",
    "    # Main training loop over epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Put the model in training mode (enables dropout, etc.)\n",
    "\n",
    "        # Iterate over training batches\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            # Compute loss for the current batch\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\n",
    "            loss.backward()        # Backpropagate the loss\n",
    "            optimizer.step()       # Apply gradient updates to model parameters\n",
    "\n",
    "            # Update token and step counters\n",
    "            tokens_seen += input_batch.numel()  # Number of tokens processed in this batch\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation at specified intervals\n",
    "            if global_step % eval_freq == 0:\n",
    "                # Evaluate model on training and validation data using `eval_iter` batches\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                # Track losses and number of tokens seen so far\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "\n",
    "                # Print progress update\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "    # Return tracked metrics for plotting or analysis\n",
    "    return train_losses, val_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c27e5c-92c4-4278-8f67-b2d82c926e92",
   "metadata": {},
   "source": [
    "Function to evaluate the GPT model’s performance on training and validation data.\n",
    "It temporarily switches the model to evaluation mode, computes average losses over a specified number of batches without tracking gradients, then restores training mode and returns the computed losses for monitoring progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a46e0798-d80d-4d1d-a9c3-66b4edbbf99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout, batchnorm updates, etc.)\n",
    "\n",
    "    # Disable gradient computation to save memory and speed up inference\n",
    "    with torch.no_grad():\n",
    "        # Evaluate training loss on a subset of `eval_iter` batches from the train_loader\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "\n",
    "        # Evaluate validation loss on a subset of `eval_iter` batches from the val_loader\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "\n",
    "    model.train()  # Return model to training mode after evaluation\n",
    "\n",
    "    # Return both training and validation loss for monitoring\n",
    "    return train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8235daa5-beae-4295-87a3-362090f4b77f",
   "metadata": {},
   "source": [
    "Implementation of a multi-head self-attention module with causal masking for autoregressive language modeling.\n",
    "It projects inputs into queries, keys, and values, splits them into multiple heads, computes scaled dot-product attention with a causal mask to prevent attending to future tokens, applies dropout, and combines the heads with a final linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76a0d9f0-18c0-4293-9149-e829b2b71ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # Ensure output dimension can be evenly divided into heads\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads    # Dimensionality of each head\n",
    "\n",
    "        # Linear layers to project input into queries, keys, and values\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # Final linear projection after attention\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout for regularization\n",
    "        # Register causal mask buffer: upper triangular (1s above the diagonal)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Project input to keys, queries, and values: shape (b, seq_len, d_out)\n",
    "        keys = self.W_key(x)     # (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Reshape and split into heads: (b, num_tokens, num_heads, head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Compute scaled dot-product attention: (b, num_heads, seq_len, seq_len)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        scaled_scores = attn_scores / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Causal mask: only attend to current and previous tokens\n",
    "        mask_bool = self.mask[:num_tokens, :num_tokens].bool() .to(x.device)  # (seq_len, seq_len)\n",
    "        mask_bool = mask_bool.unsqueeze(0).unsqueeze(0)         # (1, 1, seq_len, seq_len)\n",
    "        scaled_scores = scaled_scores.masked_fill(mask_bool, float('-inf'))\n",
    "\n",
    "        # Softmax and dropout\n",
    "        attn_weights = torch.softmax(scaled_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        # Apply attention weights to values: (b, num_heads, seq_len, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)   # (b, seq_len, num_heads, head_dim)\n",
    "\n",
    "        # Combine heads: (b, seq_len, d_out)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "\n",
    "        # Final linear projection\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf783e9-3d59-4d9f-9c91-1a41ee99d313",
   "metadata": {},
   "source": [
    "Defines key transformer building blocks:\n",
    "\t•\tGELU: Implements the Gaussian Error Linear Unit activation function with its smooth, non-linear approximation.\n",
    "\t•\tLayerNorm: Custom layer normalization that normalizes inputs per embedding dimension with learnable scale and shift parameters.\n",
    "\t•\tFeedForward: Two-layer MLP with GELU activation in between, expanding and projecting back to embedding dimension, used within transformer blocks for non-linear transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97fa85e9-19ac-4562-9655-5a7035760112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fed975-7b01-412a-9133-7548a58d10ce",
   "metadata": {},
   "source": [
    "Defines a single Transformer encoder block with pre-layer normalization and residual connections.\n",
    "It sequentially applies multi-head self-attention and a feed-forward network, each preceded by LayerNorm and followed by dropout, adding the input back via skip connections to stabilize training and improve gradient flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afd31e06-5e63-458c-933e-11698bf398c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x    # Save the input\n",
    "        x = self.norm1(x)     # Apply LayerNorm before attention\n",
    "        x = self.att(x)  # Apply multi-head self-attention Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)     # Apply dropout\n",
    "        x = x + shortcut  # Add residual connection\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x    # Save the current input again\n",
    "        x = self.norm2(x)     # Normalize before FFN\n",
    "        x = self.ff(x)     # Apply two-layer MLP\n",
    "        x = self.drop_shortcut(x)     # Dropout for regularization\n",
    "        x = x + shortcut  # Add residual connection again\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3cf65c-60b7-45cb-b29f-683a035b5014",
   "metadata": {},
   "source": [
    "GPTModel class implementing a GPT-style transformer language model.\n",
    "It includes token and positional embeddings, a stack of transformer blocks, final layer normalization, and an output linear layer projecting to vocabulary logits.\n",
    "\n",
    "The forward method computes logits for input token sequences.\n",
    "\n",
    "The generate method produces autoregressive text completions with configurable sampling options like temperature, top-k, and top-p nucleus sampling, optionally stopping at an end-of-sequence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f43bb6f-f28a-41c3-b3a0-32598f02b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self, input_ids, max_new_tokens, tokenizer=None,\n",
    "        eos_token_id=None, temperature=1.0, top_k=None, top_p=None):\n",
    "        \"\"\"\n",
    "        Generate tokens using sampling (with temperature, top-k, top-p options).\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): shape [1, T], input token IDs.\n",
    "            max_new_tokens (int): number of tokens to generate.\n",
    "            tokenizer (optional): for decoding/debugging.\n",
    "            eos_token_id (int, optional): stop generation when this token is produced.\n",
    "            temperature (float): >0. Lower = less random. 1.0 = default.\n",
    "            top_k (int, optional): only sample from top-k logits.\n",
    "            top_p (float, optional): nucleus sampling threshold (0 < top_p <= 1).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: output token IDs of shape [1, T + max_new_tokens]\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context to max context length\n",
    "            context = input_ids[:, -self.pos_emb.num_embeddings:]\n",
    "    \n",
    "            # Get logits\n",
    "            logits = self(context)  # [1, T, vocab_size]\n",
    "            logits = logits[:, -1, :] / temperature  # [1, vocab_size]\n",
    "    \n",
    "            # Optionally filter logits with top-k\n",
    "            if top_k is not None:\n",
    "                top_k = min(top_k, logits.size(-1))  # safety\n",
    "                values, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < values[:, [-1]]] = float('-inf')\n",
    "            # Optionally filter logits with top-p\n",
    "            if top_p is not None and 0 < top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
    "    \n",
    "                # Remove tokens with cumulative prob > top_p\n",
    "                sorted_mask = cumulative_probs > top_p\n",
    "                # Shift mask to include the first token above the threshold\n",
    "                sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n",
    "                sorted_mask[..., 0] = 0\n",
    "    \n",
    "                # Set logits of removed tokens to -inf\n",
    "                indices_to_remove = sorted_mask.scatter(1, sorted_indices, sorted_mask)\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "            # Sample next token from filtered distribution\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # [1, 1]\n",
    "    \n",
    "            # Append to input\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    \n",
    "            # Optionally stop at eos_token\n",
    "            if eos_token_id is not None and next_token.item() == eos_token_id:\n",
    "                break\n",
    "    \n",
    "        return input_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ab964-0d1d-47fd-a0b6-44be97ea0c4c",
   "metadata": {},
   "source": [
    "Code to initialize and train the GPT model.\n",
    "It sets a fixed random seed for reproducibility, instantiates the model with the 124M config, moves it to the specified device, sets up the AdamW optimizer with learning rate and weight decay, then trains the model for 10 epochs with periodic evaluation every 20 steps using 5 batches for loss estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34aee974-7554-4f7c-8621-3f217a0cac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.823, Val loss 9.792\n",
      "Ep 1 (Step 000020): Train loss 7.241, Val loss 6.975\n",
      "Ep 1 (Step 000040): Train loss 7.151, Val loss 6.587\n",
      "Ep 1 (Step 000060): Train loss 6.724, Val loss 6.370\n",
      "Ep 1 (Step 000080): Train loss 6.778, Val loss 6.290\n",
      "Ep 1 (Step 000100): Train loss 6.539, Val loss 6.197\n",
      "Ep 1 (Step 000120): Train loss 6.401, Val loss 6.073\n",
      "Ep 1 (Step 000140): Train loss 6.500, Val loss 6.009\n",
      "Ep 1 (Step 000160): Train loss 6.389, Val loss 5.928\n",
      "Ep 1 (Step 000180): Train loss 6.546, Val loss 5.938\n",
      "Ep 1 (Step 000200): Train loss 6.119, Val loss 5.899\n",
      "Ep 1 (Step 000220): Train loss 6.186, Val loss 5.849\n",
      "Ep 1 (Step 000240): Train loss 5.799, Val loss 5.795\n",
      "Ep 1 (Step 000260): Train loss 6.367, Val loss 5.757\n",
      "Ep 1 (Step 000280): Train loss 6.217, Val loss 5.707\n",
      "Ep 1 (Step 000300): Train loss 5.882, Val loss 5.665\n",
      "Ep 1 (Step 000320): Train loss 6.189, Val loss 5.665\n",
      "Ep 1 (Step 000340): Train loss 6.087, Val loss 5.646\n",
      "Ep 1 (Step 000360): Train loss 5.863, Val loss 5.622\n",
      "Ep 1 (Step 000380): Train loss 5.912, Val loss 5.611\n",
      "Ep 1 (Step 000400): Train loss 5.844, Val loss 5.595\n",
      "Ep 1 (Step 000420): Train loss 5.955, Val loss 5.575\n",
      "Ep 1 (Step 000440): Train loss 5.900, Val loss 5.589\n",
      "Ep 1 (Step 000460): Train loss 5.759, Val loss 5.602\n",
      "Ep 1 (Step 000480): Train loss 5.777, Val loss 5.581\n",
      "Ep 1 (Step 000500): Train loss 5.773, Val loss 5.598\n",
      "Ep 1 (Step 000520): Train loss 5.808, Val loss 5.564\n",
      "Ep 1 (Step 000540): Train loss 5.535, Val loss 5.473\n",
      "Ep 1 (Step 000560): Train loss 5.866, Val loss 5.525\n",
      "Ep 1 (Step 000580): Train loss 5.620, Val loss 5.483\n",
      "Ep 2 (Step 000600): Train loss 5.679, Val loss 5.509\n",
      "Ep 2 (Step 000620): Train loss 5.791, Val loss 5.475\n",
      "Ep 2 (Step 000640): Train loss 5.795, Val loss 5.448\n",
      "Ep 2 (Step 000660): Train loss 5.721, Val loss 5.483\n",
      "Ep 2 (Step 000680): Train loss 5.672, Val loss 5.481\n",
      "Ep 2 (Step 000700): Train loss 5.913, Val loss 5.427\n",
      "Ep 2 (Step 000720): Train loss 5.776, Val loss 5.451\n",
      "Ep 2 (Step 000740): Train loss 5.541, Val loss 5.414\n",
      "Ep 2 (Step 000760): Train loss 5.505, Val loss 5.367\n",
      "Ep 2 (Step 000780): Train loss 5.736, Val loss 5.443\n",
      "Ep 2 (Step 000800): Train loss 5.642, Val loss 5.434\n",
      "Ep 2 (Step 000820): Train loss 5.314, Val loss 5.433\n",
      "Ep 2 (Step 000840): Train loss 5.829, Val loss 5.426\n",
      "Ep 2 (Step 000860): Train loss 5.517, Val loss 5.390\n",
      "Ep 2 (Step 000880): Train loss 5.239, Val loss 5.405\n",
      "Ep 2 (Step 000900): Train loss 5.306, Val loss 5.383\n",
      "Ep 2 (Step 000920): Train loss 5.385, Val loss 5.408\n",
      "Ep 2 (Step 000940): Train loss 5.377, Val loss 5.340\n",
      "Ep 2 (Step 000960): Train loss 5.280, Val loss 5.361\n",
      "Ep 2 (Step 000980): Train loss 5.444, Val loss 5.341\n",
      "Ep 2 (Step 001000): Train loss 5.396, Val loss 5.349\n",
      "Ep 2 (Step 001020): Train loss 5.445, Val loss 5.357\n",
      "Ep 2 (Step 001040): Train loss 5.069, Val loss 5.331\n",
      "Ep 2 (Step 001060): Train loss 5.434, Val loss 5.309\n",
      "Ep 2 (Step 001080): Train loss 5.184, Val loss 5.325\n",
      "Ep 2 (Step 001100): Train loss 5.561, Val loss 5.297\n",
      "Ep 2 (Step 001120): Train loss 5.448, Val loss 5.335\n",
      "Ep 2 (Step 001140): Train loss 5.273, Val loss 5.283\n",
      "Ep 2 (Step 001160): Train loss 5.163, Val loss 5.340\n",
      "Ep 3 (Step 001180): Train loss 5.317, Val loss 5.311\n",
      "Ep 3 (Step 001200): Train loss 5.116, Val loss 5.306\n",
      "Ep 3 (Step 001220): Train loss 5.377, Val loss 5.305\n",
      "Ep 3 (Step 001240): Train loss 5.260, Val loss 5.335\n",
      "Ep 3 (Step 001260): Train loss 5.141, Val loss 5.297\n",
      "Ep 3 (Step 001280): Train loss 5.516, Val loss 5.311\n",
      "Ep 3 (Step 001300): Train loss 5.196, Val loss 5.303\n",
      "Ep 3 (Step 001320): Train loss 5.359, Val loss 5.282\n",
      "Ep 3 (Step 001340): Train loss 5.273, Val loss 5.298\n",
      "Ep 3 (Step 001360): Train loss 5.245, Val loss 5.317\n",
      "Ep 3 (Step 001380): Train loss 5.239, Val loss 5.326\n",
      "Ep 3 (Step 001400): Train loss 5.250, Val loss 5.266\n",
      "Ep 3 (Step 001420): Train loss 5.370, Val loss 5.226\n",
      "Ep 3 (Step 001440): Train loss 5.263, Val loss 5.275\n",
      "Ep 3 (Step 001460): Train loss 4.718, Val loss 5.319\n",
      "Ep 3 (Step 001480): Train loss 5.289, Val loss 5.261\n",
      "Ep 3 (Step 001500): Train loss 5.133, Val loss 5.322\n",
      "Ep 3 (Step 001520): Train loss 4.980, Val loss 5.259\n",
      "Ep 3 (Step 001540): Train loss 5.256, Val loss 5.291\n",
      "Ep 3 (Step 001560): Train loss 5.148, Val loss 5.293\n",
      "Ep 3 (Step 001580): Train loss 5.239, Val loss 5.221\n",
      "Ep 3 (Step 001600): Train loss 5.236, Val loss 5.247\n",
      "Ep 3 (Step 001620): Train loss 5.029, Val loss 5.233\n",
      "Ep 3 (Step 001640): Train loss 4.970, Val loss 5.270\n",
      "Ep 3 (Step 001660): Train loss 5.169, Val loss 5.242\n",
      "Ep 3 (Step 001680): Train loss 5.100, Val loss 5.212\n",
      "Ep 3 (Step 001700): Train loss 5.282, Val loss 5.178\n",
      "Ep 3 (Step 001720): Train loss 4.671, Val loss 5.204\n",
      "Ep 3 (Step 001740): Train loss 4.874, Val loss 5.170\n",
      "Ep 4 (Step 001760): Train loss 4.656, Val loss 5.237\n",
      "Ep 4 (Step 001780): Train loss 5.007, Val loss 5.247\n",
      "Ep 4 (Step 001800): Train loss 4.845, Val loss 5.296\n",
      "Ep 4 (Step 001820): Train loss 5.007, Val loss 5.295\n",
      "Ep 4 (Step 001840): Train loss 4.896, Val loss 5.254\n",
      "Ep 4 (Step 001860): Train loss 5.143, Val loss 5.271\n",
      "Ep 4 (Step 001880): Train loss 4.945, Val loss 5.286\n",
      "Ep 4 (Step 001900): Train loss 4.979, Val loss 5.301\n",
      "Ep 4 (Step 001920): Train loss 4.907, Val loss 5.261\n",
      "Ep 4 (Step 001940): Train loss 4.747, Val loss 5.238\n",
      "Ep 4 (Step 001960): Train loss 4.805, Val loss 5.243\n",
      "Ep 4 (Step 001980): Train loss 4.882, Val loss 5.213\n",
      "Ep 4 (Step 002000): Train loss 4.739, Val loss 5.207\n",
      "Ep 4 (Step 002020): Train loss 4.951, Val loss 5.213\n",
      "Ep 4 (Step 002040): Train loss 4.710, Val loss 5.225\n",
      "Ep 4 (Step 002060): Train loss 4.813, Val loss 5.253\n",
      "Ep 4 (Step 002080): Train loss 4.924, Val loss 5.234\n",
      "Ep 4 (Step 002100): Train loss 4.863, Val loss 5.234\n",
      "Ep 4 (Step 002120): Train loss 4.738, Val loss 5.213\n",
      "Ep 4 (Step 002140): Train loss 4.657, Val loss 5.179\n",
      "Ep 4 (Step 002160): Train loss 4.814, Val loss 5.201\n",
      "Ep 4 (Step 002180): Train loss 4.728, Val loss 5.157\n",
      "Ep 4 (Step 002200): Train loss 4.672, Val loss 5.183\n",
      "Ep 4 (Step 002220): Train loss 4.591, Val loss 5.230\n",
      "Ep 4 (Step 002240): Train loss 4.740, Val loss 5.294\n",
      "Ep 4 (Step 002260): Train loss 4.956, Val loss 5.230\n",
      "Ep 4 (Step 002280): Train loss 4.855, Val loss 5.185\n",
      "Ep 4 (Step 002300): Train loss 4.755, Val loss 5.231\n",
      "Ep 4 (Step 002320): Train loss 4.693, Val loss 5.213\n",
      "Ep 5 (Step 002340): Train loss 4.568, Val loss 5.280\n",
      "Ep 5 (Step 002360): Train loss 4.559, Val loss 5.278\n",
      "Ep 5 (Step 002380): Train loss 4.284, Val loss 5.253\n",
      "Ep 5 (Step 002400): Train loss 4.736, Val loss 5.267\n",
      "Ep 5 (Step 002420): Train loss 4.704, Val loss 5.285\n",
      "Ep 5 (Step 002440): Train loss 4.605, Val loss 5.245\n",
      "Ep 5 (Step 002460): Train loss 4.441, Val loss 5.261\n",
      "Ep 5 (Step 002480): Train loss 4.459, Val loss 5.278\n",
      "Ep 5 (Step 002500): Train loss 4.633, Val loss 5.284\n",
      "Ep 5 (Step 002520): Train loss 4.491, Val loss 5.263\n",
      "Ep 5 (Step 002540): Train loss 4.253, Val loss 5.309\n",
      "Ep 5 (Step 002560): Train loss 4.610, Val loss 5.318\n",
      "Ep 5 (Step 002580): Train loss 4.367, Val loss 5.315\n",
      "Ep 5 (Step 002600): Train loss 4.439, Val loss 5.253\n",
      "Ep 5 (Step 002620): Train loss 4.341, Val loss 5.285\n",
      "Ep 5 (Step 002640): Train loss 4.298, Val loss 5.231\n",
      "Ep 5 (Step 002660): Train loss 4.566, Val loss 5.268\n",
      "Ep 5 (Step 002680): Train loss 4.480, Val loss 5.269\n",
      "Ep 5 (Step 002700): Train loss 4.635, Val loss 5.261\n",
      "Ep 5 (Step 002720): Train loss 4.414, Val loss 5.248\n",
      "Ep 5 (Step 002740): Train loss 4.353, Val loss 5.225\n",
      "Ep 5 (Step 002760): Train loss 4.419, Val loss 5.271\n",
      "Ep 5 (Step 002780): Train loss 4.568, Val loss 5.248\n",
      "Ep 5 (Step 002800): Train loss 4.155, Val loss 5.228\n",
      "Ep 5 (Step 002820): Train loss 4.490, Val loss 5.289\n",
      "Ep 5 (Step 002840): Train loss 4.456, Val loss 5.260\n",
      "Ep 5 (Step 002860): Train loss 4.296, Val loss 5.254\n",
      "Ep 5 (Step 002880): Train loss 4.369, Val loss 5.292\n",
      "Ep 5 (Step 002900): Train loss 4.452, Val loss 5.246\n",
      "Ep 6 (Step 002920): Train loss 4.275, Val loss 5.283\n",
      "Ep 6 (Step 002940): Train loss 4.316, Val loss 5.323\n",
      "Ep 6 (Step 002960): Train loss 4.210, Val loss 5.369\n",
      "Ep 6 (Step 002980): Train loss 4.241, Val loss 5.325\n",
      "Ep 6 (Step 003000): Train loss 4.376, Val loss 5.356\n",
      "Ep 6 (Step 003020): Train loss 4.189, Val loss 5.411\n",
      "Ep 6 (Step 003040): Train loss 4.287, Val loss 5.371\n",
      "Ep 6 (Step 003060): Train loss 3.997, Val loss 5.372\n",
      "Ep 6 (Step 003080): Train loss 4.310, Val loss 5.351\n",
      "Ep 6 (Step 003100): Train loss 4.020, Val loss 5.316\n",
      "Ep 6 (Step 003120): Train loss 3.901, Val loss 5.312\n",
      "Ep 6 (Step 003140): Train loss 4.114, Val loss 5.338\n",
      "Ep 6 (Step 003160): Train loss 4.033, Val loss 5.327\n",
      "Ep 6 (Step 003180): Train loss 4.282, Val loss 5.316\n",
      "Ep 6 (Step 003200): Train loss 4.330, Val loss 5.344\n",
      "Ep 6 (Step 003220): Train loss 4.081, Val loss 5.356\n",
      "Ep 6 (Step 003240): Train loss 4.135, Val loss 5.331\n",
      "Ep 6 (Step 003260): Train loss 4.149, Val loss 5.319\n",
      "Ep 6 (Step 003280): Train loss 4.335, Val loss 5.379\n",
      "Ep 6 (Step 003300): Train loss 3.861, Val loss 5.339\n",
      "Ep 6 (Step 003320): Train loss 4.107, Val loss 5.329\n",
      "Ep 6 (Step 003340): Train loss 4.235, Val loss 5.355\n",
      "Ep 6 (Step 003360): Train loss 3.836, Val loss 5.333\n",
      "Ep 6 (Step 003380): Train loss 3.960, Val loss 5.311\n",
      "Ep 6 (Step 003400): Train loss 3.919, Val loss 5.296\n",
      "Ep 6 (Step 003420): Train loss 3.916, Val loss 5.284\n",
      "Ep 6 (Step 003440): Train loss 3.914, Val loss 5.348\n",
      "Ep 6 (Step 003460): Train loss 3.974, Val loss 5.345\n",
      "Ep 6 (Step 003480): Train loss 4.050, Val loss 5.326\n",
      "Ep 7 (Step 003500): Train loss 3.849, Val loss 5.391\n",
      "Ep 7 (Step 003520): Train loss 3.793, Val loss 5.430\n",
      "Ep 7 (Step 003540): Train loss 4.072, Val loss 5.450\n",
      "Ep 7 (Step 003560): Train loss 3.808, Val loss 5.445\n",
      "Ep 7 (Step 003580): Train loss 3.517, Val loss 5.439\n",
      "Ep 7 (Step 003600): Train loss 3.865, Val loss 5.469\n",
      "Ep 7 (Step 003620): Train loss 3.744, Val loss 5.416\n",
      "Ep 7 (Step 003640): Train loss 3.736, Val loss 5.391\n",
      "Ep 7 (Step 003660): Train loss 4.099, Val loss 5.408\n",
      "Ep 7 (Step 003680): Train loss 3.810, Val loss 5.435\n",
      "Ep 7 (Step 003700): Train loss 3.512, Val loss 5.436\n",
      "Ep 7 (Step 003720): Train loss 3.483, Val loss 5.430\n",
      "Ep 7 (Step 003740): Train loss 3.800, Val loss 5.464\n",
      "Ep 7 (Step 003760): Train loss 3.928, Val loss 5.407\n",
      "Ep 7 (Step 003780): Train loss 3.711, Val loss 5.444\n",
      "Ep 7 (Step 003800): Train loss 3.737, Val loss 5.418\n",
      "Ep 7 (Step 003820): Train loss 3.687, Val loss 5.429\n",
      "Ep 7 (Step 003840): Train loss 3.808, Val loss 5.411\n",
      "Ep 7 (Step 003860): Train loss 3.614, Val loss 5.434\n",
      "Ep 7 (Step 003880): Train loss 3.793, Val loss 5.498\n",
      "Ep 7 (Step 003900): Train loss 3.807, Val loss 5.477\n",
      "Ep 7 (Step 003920): Train loss 3.368, Val loss 5.457\n",
      "Ep 7 (Step 003940): Train loss 3.508, Val loss 5.484\n",
      "Ep 7 (Step 003960): Train loss 3.598, Val loss 5.456\n",
      "Ep 7 (Step 003980): Train loss 3.623, Val loss 5.458\n",
      "Ep 7 (Step 004000): Train loss 3.545, Val loss 5.458\n",
      "Ep 7 (Step 004020): Train loss 3.631, Val loss 5.424\n",
      "Ep 7 (Step 004040): Train loss 3.430, Val loss 5.443\n",
      "Ep 7 (Step 004060): Train loss 3.456, Val loss 5.433\n",
      "Ep 8 (Step 004080): Train loss 3.433, Val loss 5.513\n",
      "Ep 8 (Step 004100): Train loss 3.521, Val loss 5.497\n",
      "Ep 8 (Step 004120): Train loss 3.523, Val loss 5.549\n",
      "Ep 8 (Step 004140): Train loss 3.510, Val loss 5.575\n",
      "Ep 8 (Step 004160): Train loss 3.314, Val loss 5.592\n",
      "Ep 8 (Step 004180): Train loss 3.537, Val loss 5.615\n",
      "Ep 8 (Step 004200): Train loss 3.543, Val loss 5.582\n",
      "Ep 8 (Step 004220): Train loss 3.379, Val loss 5.601\n",
      "Ep 8 (Step 004240): Train loss 3.369, Val loss 5.569\n",
      "Ep 8 (Step 004260): Train loss 3.506, Val loss 5.595\n",
      "Ep 8 (Step 004280): Train loss 3.315, Val loss 5.590\n",
      "Ep 8 (Step 004300): Train loss 3.437, Val loss 5.577\n",
      "Ep 8 (Step 004320): Train loss 3.441, Val loss 5.627\n",
      "Ep 8 (Step 004340): Train loss 3.369, Val loss 5.651\n",
      "Ep 8 (Step 004360): Train loss 3.294, Val loss 5.633\n",
      "Ep 8 (Step 004380): Train loss 3.271, Val loss 5.636\n",
      "Ep 8 (Step 004400): Train loss 3.506, Val loss 5.627\n",
      "Ep 8 (Step 004420): Train loss 3.296, Val loss 5.576\n",
      "Ep 8 (Step 004440): Train loss 3.104, Val loss 5.581\n",
      "Ep 8 (Step 004460): Train loss 3.574, Val loss 5.580\n",
      "Ep 8 (Step 004480): Train loss 3.040, Val loss 5.565\n",
      "Ep 8 (Step 004500): Train loss 3.141, Val loss 5.618\n",
      "Ep 8 (Step 004520): Train loss 3.358, Val loss 5.592\n",
      "Ep 8 (Step 004540): Train loss 3.216, Val loss 5.589\n",
      "Ep 8 (Step 004560): Train loss 3.302, Val loss 5.562\n",
      "Ep 8 (Step 004580): Train loss 3.073, Val loss 5.545\n",
      "Ep 8 (Step 004600): Train loss 3.112, Val loss 5.530\n",
      "Ep 8 (Step 004620): Train loss 3.220, Val loss 5.521\n",
      "Ep 8 (Step 004640): Train loss 2.955, Val loss 5.538\n",
      "Ep 9 (Step 004660): Train loss 3.173, Val loss 5.589\n",
      "Ep 9 (Step 004680): Train loss 2.845, Val loss 5.628\n",
      "Ep 9 (Step 004700): Train loss 3.176, Val loss 5.652\n",
      "Ep 9 (Step 004720): Train loss 2.992, Val loss 5.679\n",
      "Ep 9 (Step 004740): Train loss 3.270, Val loss 5.694\n",
      "Ep 9 (Step 004760): Train loss 2.758, Val loss 5.693\n",
      "Ep 9 (Step 004780): Train loss 2.990, Val loss 5.735\n",
      "Ep 9 (Step 004800): Train loss 3.028, Val loss 5.711\n",
      "Ep 9 (Step 004820): Train loss 3.015, Val loss 5.722\n",
      "Ep 9 (Step 004840): Train loss 2.643, Val loss 5.720\n",
      "Ep 9 (Step 004860): Train loss 3.321, Val loss 5.698\n",
      "Ep 9 (Step 004880): Train loss 2.769, Val loss 5.760\n",
      "Ep 9 (Step 004900): Train loss 2.905, Val loss 5.729\n",
      "Ep 9 (Step 004920): Train loss 2.776, Val loss 5.713\n",
      "Ep 9 (Step 004940): Train loss 2.981, Val loss 5.698\n",
      "Ep 9 (Step 004960): Train loss 2.758, Val loss 5.731\n",
      "Ep 9 (Step 004980): Train loss 2.797, Val loss 5.742\n",
      "Ep 9 (Step 005000): Train loss 3.019, Val loss 5.756\n",
      "Ep 9 (Step 005020): Train loss 2.563, Val loss 5.726\n",
      "Ep 9 (Step 005040): Train loss 2.573, Val loss 5.758\n",
      "Ep 9 (Step 005060): Train loss 2.814, Val loss 5.766\n",
      "Ep 9 (Step 005080): Train loss 2.716, Val loss 5.779\n",
      "Ep 9 (Step 005100): Train loss 2.858, Val loss 5.762\n",
      "Ep 9 (Step 005120): Train loss 2.691, Val loss 5.701\n",
      "Ep 9 (Step 005140): Train loss 2.664, Val loss 5.714\n",
      "Ep 9 (Step 005160): Train loss 2.774, Val loss 5.762\n",
      "Ep 9 (Step 005180): Train loss 2.735, Val loss 5.789\n",
      "Ep 9 (Step 005200): Train loss 2.567, Val loss 5.796\n",
      "Ep 9 (Step 005220): Train loss 2.842, Val loss 5.783\n",
      "Ep 10 (Step 005240): Train loss 2.509, Val loss 5.796\n",
      "Ep 10 (Step 005260): Train loss 2.610, Val loss 5.843\n",
      "Ep 10 (Step 005280): Train loss 2.630, Val loss 5.821\n",
      "Ep 10 (Step 005300): Train loss 2.404, Val loss 5.866\n",
      "Ep 10 (Step 005320): Train loss 2.598, Val loss 5.914\n",
      "Ep 10 (Step 005340): Train loss 2.777, Val loss 5.865\n",
      "Ep 10 (Step 005360): Train loss 2.542, Val loss 5.917\n",
      "Ep 10 (Step 005380): Train loss 2.635, Val loss 5.898\n",
      "Ep 10 (Step 005400): Train loss 2.499, Val loss 5.926\n",
      "Ep 10 (Step 005420): Train loss 2.497, Val loss 5.920\n",
      "Ep 10 (Step 005440): Train loss 2.068, Val loss 5.924\n",
      "Ep 10 (Step 005460): Train loss 2.617, Val loss 5.929\n",
      "Ep 10 (Step 005480): Train loss 2.317, Val loss 5.923\n",
      "Ep 10 (Step 005500): Train loss 2.710, Val loss 5.947\n",
      "Ep 10 (Step 005520): Train loss 2.613, Val loss 6.009\n",
      "Ep 10 (Step 005540): Train loss 2.449, Val loss 5.966\n",
      "Ep 10 (Step 005560): Train loss 2.496, Val loss 5.989\n",
      "Ep 10 (Step 005580): Train loss 2.192, Val loss 5.982\n",
      "Ep 10 (Step 005600): Train loss 2.528, Val loss 5.966\n",
      "Ep 10 (Step 005620): Train loss 2.459, Val loss 6.000\n",
      "Ep 10 (Step 005640): Train loss 2.328, Val loss 5.952\n",
      "Ep 10 (Step 005660): Train loss 2.272, Val loss 6.019\n",
      "Ep 10 (Step 005680): Train loss 2.184, Val loss 5.960\n",
      "Ep 10 (Step 005700): Train loss 2.176, Val loss 5.976\n",
      "Ep 10 (Step 005720): Train loss 2.184, Val loss 5.957\n",
      "Ep 10 (Step 005740): Train loss 2.084, Val loss 5.890\n",
      "Ep 10 (Step 005760): Train loss 2.196, Val loss 5.935\n",
      "Ep 10 (Step 005780): Train loss 2.290, Val loss 5.935\n",
      "Ep 10 (Step 005800): Train loss 2.328, Val loss 5.902\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "device='cpu'\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, track_tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=20, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae376bfc-e40f-44a3-8367-e966d69cccbe",
   "metadata": {},
   "source": [
    "Function to visualize training and validation loss curves over training progress.\n",
    "It plots losses against the number of tokens processed, adding labels, grid, and legends for clarity, helping to monitor model convergence and detect overfitting or underfitting trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44e2e80b-ff48-49bf-b45c-2808aa702a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, val_losses, track_tokens_seen, eval_freq=20):\n",
    "    # Compute x-axis (training steps)\n",
    "    steps = list(range(0, eval_freq * len(val_losses), eval_freq))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    #plt.plot(range(len(train_losses)), train_losses, label='Train Loss', alpha=0.7)\n",
    "    #plt.plot(steps, val_losses, label='Validation Loss', marker='o')\n",
    "    plt.plot(track_tokens_seen, train_losses, label='Train Loss')\n",
    "    plt.plot(track_tokens_seen, val_losses, label='Validation Loss', marker='o')\n",
    "\n",
    "    plt.title(\"Training and Validation Loss Over Time\")\n",
    "    plt.xlabel(\"Training Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_losses(train_losses, val_losses, track_tokens_seen, eval_freq=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60324bcf-74bf-4c56-aa3c-009d9d5e1f9e",
   "metadata": {},
   "source": [
    "Code to generate text continuation using the trained GPT model.\n",
    "It tokenizes a prompt, runs autoregressive generation with sampling parameters (temperature, top-k, top-p), stops at the end-of-text token, and decodes the output token IDs back into readable text for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b81f2710-8a14-4070-9ea6-031adce4ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky was clear by no means of singing away from\n",
      "not been slowly; how he can have gone down in the most athletic\n",
      "the mouth; but it was, and\n",
      "apparently hope they seemed as far wound hitherto been freed or malice; yet no longer as\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\") # Load tokenizer\n",
    "\n",
    "# Prompt\n",
    "prompt = \"The sky was clear\"\n",
    "token_ids = tokenizer.encode(prompt)\n",
    "input_ids = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "\n",
    "# Generate\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.9,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    eos_token_id=tokenizer.eot_token  # or 50256\n",
    ")\n",
    "\n",
    "# Decode (tiktoken does not have decode with skip_special_tokens)\n",
    "output_text = tokenizer.decode(output_ids[0].tolist())\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a47d9b-33aa-437b-a6ff-31a8670b6f43",
   "metadata": {},
   "source": [
    "Function to save a custom PyTorch GPT model’s weights and configuration to disk.\n",
    "It creates a directory (if needed), saves the model’s state dictionary as a .pt file, and writes the model configuration dictionary as a formatted JSON file, enabling easy future loading and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "284c6b62-63a5-4b72-884a-436b716af804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and config saved to: saved_gpt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_custom_model(model, config, save_dir=\"custom_gpt_model\"):\n",
    "    \"\"\"\n",
    "    Save a custom PyTorch GPT model and its configuration.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Your custom GPT model (e.g., GPTModel).\n",
    "        config (dict): Configuration dictionary used to build the model.\n",
    "        save_dir (str): Directory to save model files.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Save model weights\n",
    "    weights_path = os.path.join(save_dir, \"model_weights.pt\")\n",
    "    torch.save(model.state_dict(), weights_path)\n",
    "    \n",
    "    # Save model config\n",
    "    config_path = os.path.join(save_dir, \"config.json\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    print(f\"Model and config saved to: {save_dir}\")\n",
    "\n",
    "save_custom_model(model, GPT_CONFIG_124M, save_dir=\"saved_gpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd88340-159a-4ec9-9c17-a75d3a4b75a2",
   "metadata": {},
   "source": [
    "Function to load a custom PyTorch GPT model from saved files.\n",
    "It reads the configuration JSON, reconstructs the model instance, loads saved weights, moves the model to the specified device (CPU or GPU), and returns both the model and its config for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec137092-b654-4126-a8dd-64a827428583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_custom_model(model_class, save_dir=\"custom_gpt_model\", device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Load a custom PyTorch GPT model and its weights.\n",
    "\n",
    "    Args:\n",
    "        model_class: The class used to define the model (e.g., GPTModel).\n",
    "        save_dir (str): Directory containing model_weights.pt and config.json.\n",
    "        device (str): \"cpu\" or \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        model (torch.nn.Module): The loaded model.\n",
    "        config (dict): The configuration used to initialize it.\n",
    "    \"\"\"\n",
    "    # Load config\n",
    "    config_path = os.path.join(save_dir, \"config.json\")\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "    # Reconstruct model from config\n",
    "    model = model_class(config)\n",
    "\n",
    "    # Load weights\n",
    "    weights_path = os.path.join(save_dir, \"model_weights.pt\")\n",
    "    state_dict = torch.load(weights_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Move to device\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"Model loaded from: {save_dir}\")\n",
    "    return model, config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16edf551-18e2-43df-915e-7a7951c3290d",
   "metadata": {},
   "source": [
    "Code snippet to load the saved custom GPT model onto the available device (GPU if possible, otherwise CPU).\n",
    "It uses the previously defined load function, then prints the model architecture to confirm successful loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e0d59fa-75bf-4877-8516-993637cd1025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: saved_gpt\n",
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(256, 768)\n",
      "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load your custom GPT model\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, config = load_custom_model(GPTModel, save_dir=\"saved_gpt\", device=device)\n",
    "\n",
    "# Confirm it's working\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e38953-a916-4648-82d1-3db609f21280",
   "metadata": {},
   "source": [
    "Code to generate text continuation from a loaded GPT model using a new prompt.\n",
    "It encodes the prompt, generates tokens autoregressively with sampling controls, decodes the output tokens back to text, and prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d8979d8-3b5c-4d10-a89b-6ac50dc5cf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every journey starts with his own\n",
      "unend, the North; the Captain Ahab and with a hitching hands, when the Virgin, and then the White Whale at his ivory\n",
      "wholly aspect of an additional blue, literallyage, till\n",
      "angu, that bro\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\") # Load tokenizer\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Every journey starts with\"\n",
    "token_ids = tokenizer.encode(prompt)\n",
    "input_ids = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "\n",
    "# Generate\n",
    "output_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.9,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    eos_token_id=tokenizer.eot_token  # or 50256\n",
    ")\n",
    "\n",
    "# Decode (tiktoken does not have decode with skip_special_tokens)\n",
    "output_text = tokenizer.decode(output_ids[0].tolist())\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b85da645-db31-4090-bcc2-bba090e732c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting packaging>=20.0 (from transformers)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.7.34-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading transformers-4.54.1-py3-none-any.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Downloading regex-2025.7.34-cp313-cp313-macosx_11_0_arm64.whl (285 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp313-cp313-macosx_10_13_universal2.whl (199 kB)\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, packaging, numpy, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2025.7.14 charset_normalizer-3.4.2 filelock-3.18.0 fsspec-2025.7.0 hf-xet-1.1.5 huggingface-hub-0.34.3 idna-3.10 numpy-2.3.2 packaging-25.0 pyyaml-6.0.2 regex-2025.7.34 requests-2.32.4 safetensors-0.5.3 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.54.1 typing-extensions-4.14.1 urllib3-2.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65f545ac-d814-4f48-aa95-93631d765758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Using cached transformers-4.54.1-py3-none-any.whl (11.2 MB)\n",
      "Using cached huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Installing collected packages: tqdm, safetensors, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed hf-xet-1.1.5 huggingface-hub-0.34.3 safetensors-0.5.3 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.54.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c154ec09-f687-4d9c-921b-fd9c59aaa9a3",
   "metadata": {},
   "source": [
    "Code to load pretrained weights from the Hugging Face GPT-2 model into a custom GPT implementation.\n",
    "It defines model configurations for different GPT-2 sizes, initializes a Hugging Face GPT2Model, and copies parameters layer-by-layer—splitting combined QKV weights/biases, transposing where necessary, and assigning them to corresponding custom model attributes with shape checks to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e55f2e8-1cb1-460c-82cd-b67919fa3514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 50257, 'context_length': 1024, 'drop_rate': 0.0, 'qkv_bias': True, 'emb_dim': 768, 'n_layers': 12, 'n_heads': 12}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import GPT2Model\n",
    "\n",
    "# allowed model names\n",
    "model_names = {\n",
    "    \"gpt2-small (124M)\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium (355M)\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large (774M)\": \"openai-community/gpt2-large\",\n",
    "    \"gpt2-xl (1558M)\": \"openai-community/gpt2-xl\"\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "gpt_hf = GPT2Model.from_pretrained(model_names[CHOOSE_MODEL], cache_dir=\"checkpoints\")\n",
    "gpt_hf.eval()\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": True        # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "print(BASE_CONFIG)\n",
    "\n",
    "def assign_check(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(right.clone().detach())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_weights(gpt, gpt_hf):\n",
    "    d = gpt_hf.state_dict()\n",
    "\n",
    "    gpt.pos_emb.weight = assign_check(gpt.pos_emb.weight, d[\"wpe.weight\"])\n",
    "    gpt.tok_emb.weight = assign_check(gpt.tok_emb.weight, d[\"wte.weight\"])\n",
    "    \n",
    "    for b in range(BASE_CONFIG[\"n_layers\"]):\n",
    "        q_w, k_w, v_w = np.split(d[f\"h.{b}.attn.c_attn.weight\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign_check(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign_check(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign_check(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "        q_b, k_b, v_b = np.split(d[f\"h.{b}.attn.c_attn.bias\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign_check(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign_check(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign_check(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "    \n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign_check(gpt.trf_blocks[b].att.out_proj.weight, d[f\"h.{b}.attn.c_proj.weight\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign_check(gpt.trf_blocks[b].att.out_proj.bias, d[f\"h.{b}.attn.c_proj.bias\"])\n",
    "    \n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign_check(gpt.trf_blocks[b].ff.layers[0].weight, d[f\"h.{b}.mlp.c_fc.weight\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign_check(gpt.trf_blocks[b].ff.layers[0].bias, d[f\"h.{b}.mlp.c_fc.bias\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign_check(gpt.trf_blocks[b].ff.layers[2].weight, d[f\"h.{b}.mlp.c_proj.weight\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign_check(gpt.trf_blocks[b].ff.layers[2].bias, d[f\"h.{b}.mlp.c_proj.bias\"])\n",
    "        gpt.trf_blocks[b].norm1.scale = assign_check(gpt.trf_blocks[b].norm1.scale, d[f\"h.{b}.ln_1.weight\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign_check(gpt.trf_blocks[b].norm1.shift, d[f\"h.{b}.ln_1.bias\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign_check(gpt.trf_blocks[b].norm2.scale, d[f\"h.{b}.ln_2.weight\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign_check(gpt.trf_blocks[b].norm2.shift, d[f\"h.{b}.ln_2.bias\"])\n",
    "    \n",
    "        gpt.final_norm.scale = assign_check(gpt.final_norm.scale, d[f\"ln_f.weight\"])\n",
    "        gpt.final_norm.shift = assign_check(gpt.final_norm.shift, d[f\"ln_f.bias\"])\n",
    "        gpt.out_head.weight = assign_check(gpt.out_head.weight, d[\"wte.weight\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f87b90-a12f-4e36-8c9d-fdc4ce1ac41c",
   "metadata": {},
   "source": [
    "Code to instantiate the custom GPT model with the loaded HF weights and move it to the appropriate device (GPU if available).\n",
    "Also includes utility functions to encode text into token ID tensors with batch dimension and decode token ID tensors back into text using the GPT-2 tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0d7cdc5-4932-4722-8acc-0684e28ba388",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTModel(BASE_CONFIG)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_weights(gpt, gpt_hf)\n",
    "gpt.to(device)\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae58000-1f62-48b2-a96d-496bbd8727f9",
   "metadata": {},
   "source": [
    "Code to generate text using the custom GPT model loaded with pretrained weights.\n",
    "It encodes a prompt, generates up to 150 new tokens with sampling options (temperature, top-k, top-p), stops at the end-of-text token, then decodes and prints the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca83ac8d-8eb9-4638-8b25-3fe8d743c253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky was clear and beautiful. The day was filled with bright stars, and the clouds were as bright as the sun. As the stars slowly came out of the clouds, they were bright and bright. When they turned and set, the sky became dim.\n",
      "\n",
      "Even when the sun shone out through the clouds, the sky was not beautiful. Even at the end of the day, they were beautiful.\n",
      "\n",
      "When the day was over, all of the people who were present were satisfied with the night, even if they were not able to see the night at all.\n",
      "\n",
      "In the morning, when the sun became a perfect blue, the sky became beautiful, even if you couldn't see it.\n",
      "\n",
      "After the sun had passed by, the sky\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "prompt = \"The sky was clear\"\n",
    "\n",
    "token_ids = tokenizer.encode(prompt)\n",
    "input_ids = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "\n",
    "output_ids = gpt.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.9,\n",
    "    top_k=40,\n",
    "    top_p=0.95,\n",
    "    eos_token_id=tokenizer.eot_token  # or 50256\n",
    ")\n",
    "\n",
    "# Decode (tiktoken does not have decode with skip_special_tokens)\n",
    "output_text = tokenizer.decode(output_ids[0].tolist())\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd83307-9e69-4500-aed4-3ea378132424",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Outputs from Models\n",
    "\n",
    "**Output from my trained GPT model:**\n",
    "\n",
    "> The sky was clear by no means of singing away from not been slowly; how he can have gone down in the most athletic the mouth; but it was, and apparently hope they seemed as far wound hitherto been freed or malice; yet no longer as\n",
    "\n",
    "**Output from the pretrained GPT-2 small model:**\n",
    "\n",
    "> The sky was clear and beautiful. The day was filled with bright stars, and the clouds were as bright as the sun. As the stars slowly came out of the clouds, they were bright and bright. When they turned and set, the sky became dim. Even when the sun shone out through the clouds, the sky was not beautiful. Even at the end of the day, they were beautiful. When the day was over, all of the people who were present were satisfied with the night, even if they were not able to see the night at all. In the morning, when the sun became a perfect blue, the sky became beautiful, even if you couldn't see it. After the sun had passed by, the sky.\n",
    "\n",
    "## Analysis and Comparison\n",
    "\n",
    "The text generated by **my trained model** is somewhat rough and doesn’t flow very well. Sentences feel broken or disconnected, and some parts lack clear meaning. This shows the model is still learning language patterns but hasn’t fully mastered them yet.\n",
    "\n",
    "In contrast, the **pretrained GPT-2 model** produces smooth, well-formed sentences. Its output is easy to read and paints a vivid picture of the sky. The sentences flow logically with richer vocabulary, making the text sound more natural and engaging.\n",
    "\n",
    "The main reason for this difference is likely because my model was trained on a smaller dataset and fewer epochs, so it hasn’t fully captured the nuances of language. The pretrained GPT-2 model has been trained on a much larger corpus and for longer, resulting in a better grasp of grammar, context, and style.\n",
    "\n",
    "Overall, my model is a promising start demonstrating the core GPT concepts, but the pretrained GPT-2 is noticeably more polished and capable of generating meaningful and coherent text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
